<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta
    name="viewport"
    content="width=device-width, initial-scale=1, viewport-fit=cover"
  />
  <title>Voice Agent — Audio Unlock</title>
  <style>
    body { font-family: -apple-system, system-ui, Segoe UI, Roboto, Helvetica, Arial, sans-serif; padding: 24px; line-height: 1.35; }
    h1 { font-size: 34px; margin: 0 0 18px; }
    button { font-size: 20px; padding: 14px 18px; margin: 8px 10px 8px 0; border-radius: 12px; border: 1px solid #c8c8c8; background: #f6f6f7; }
    button:disabled { opacity: 0.5; }
    #status { font-weight: 700; }
    #log { width: 100%; height: 360px; border: 1px solid #ddd; border-radius: 12px; padding: 16px; overflow: auto; white-space: pre-wrap; background: #fafafa; }
    .row { margin: 10px 0; }
  </style>
</head>
<body>
  <h1>Voice Agent — Audio Unlock</h1>

  <div class="row">
    <button id="beep-wa">Beep (WebAudio)</button>
    <button id="beep-tag">Beep (AudioTag)</button>
    <button id="nudge">Nudge audio</button>
  </div>

  <div class="row">
    <button id="unlock-mic">Unlock mic</button>
    <button id="start">Start voice agent</button>
    <button id="stop">Stop</button>
  </div>

  <div class="row">
    <button id="force">Force reply</button>
    <button id="test">Test sound</button>
  </div>

  <p>Status: <span id="status">idle</span></p>
  <pre id="log"></pre>

  <!-- Hidden audio element for the remote track -->
  <audio id="out" autoplay playsinline></audio>
  <!-- Tiny data URI beep for the <audio> tag path -->
  <audio id="beep" preload="auto"
    src="data:audio/wav;base64,UklGRiQAAABXQVZFZm10IBAAAAABAAEAQB8AAEAfAAABAAgAZGF0YQAAAAAAAP//AP//AP//AP//AP//AP//AP//AP//"></audio>

<script>
  // ===== helpers =====
  const $ = sel => document.querySelector(sel);
  const logBox = $('#log');
  function log(msg) {
    const s = typeof msg === 'string' ? msg : JSON.stringify(msg);
    logBox.textContent += s + '\n';
    logBox.scrollTop = logBox.scrollHeight;
  }
  function setStatus(s) { $('#status').textContent = s; }

  // ===== audio unlock tools =====
  let audioCtx;
  async function beepWebAudio(ms = 120, freq = 880) {
    try {
      if (!audioCtx) audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      if (audioCtx.state === 'suspended') await audioCtx.resume();
      const o = audioCtx.createOscillator();
      const g = audioCtx.createGain();
      o.type = 'sine';
      o.frequency.value = freq;
      o.connect(g); g.connect(audioCtx.destination);
      g.gain.setValueAtTime(0.1, audioCtx.currentTime);
      o.start();
      await new Promise(r => setTimeout(r, ms));
      o.stop();
      log('WebAudio beep');
    } catch (e) {
      log('webaudio error: ' + e.message);
    }
  }
  function beepAudioTag() {
    const tag = $('#beep');
    tag.currentTime = 0;
    tag.play().then(() => log('AudioTag beep')).catch(err => log('AudioTag play error: ' + err));
  }
  async function nudgeAudio() {
    await beepWebAudio(40, 1200);
    log('WebAudio nudge OK');
  }

  // Explicit, user-gesture microphone unlock for iOS
  async function unlockMicOnce() {
    log('requesting microphone…');
    try {
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      // we only wanted the permission; stop temp tracks
      stream.getTracks().forEach(t => t.stop());
      log('microphone OK');
      return true;
    } catch (err) {
      log('mic error: ' + (err && err.message ? err.message : err));
      return false;
    }
  }

  // ===== WebRTC to OpenAI Realtime =====
  let pc, dc, localStream;

  async function startAgent() {
    try {
      setStatus('preparing…');

      // 0) make sure output audio is unlocked
      await nudgeAudio();

      // 1) ENSURE mic permission (critical on iOS)
      const ok = await unlockMicOnce();
      if (!ok) { setStatus('idle'); return; }

      // 2) fetch a short-lived client_secret from our server
      log('fetching short-lived client_secret…');
      const sess = await fetch('/session', { method: 'POST' }).then(r => r.json());
      if (!sess || !sess.client_secret || !sess.client_secret.value) {
        log('error: no client_secret from /session'); setStatus('idle'); return;
      }
      const clientSecret = sess.client_secret.value;

      // 3) prepare local mic (real stream this time)
      localStream = await navigator.mediaDevices.getUserMedia({ audio: true });

      // 4) webRTC plumbing
      pc = new RTCPeerConnection();
      pc.onconnectionstatechange = () => log('pc state: ' + pc.connectionState);
      pc.oniceconnectionstatechange = () => log('ice state: ' + pc.iceConnectionState);

      // remote audio
      pc.ontrack = (e) => { $('#out').srcObject = e.streams[0]; };

      // send mic
      for (const track of localStream.getTracks()) pc.addTrack(track, localStream);

      // data channel for events
      dc = pc.createDataChannel('oai-events');
      dc.onopen = () => log('datachannel open');
      dc.onmessage = (ev) => log('event: ' + ev.data);

      // 5) create offer
      const offer = await pc.createOffer({ offerToReceiveAudio: true });
      await pc.setLocalDescription(offer);

      // 6) exchange SDP directly with OpenAI Realtime using the client_secret
      const model = 'gpt-4o-realtime-preview-2024-12-17';
      const sdpAnswer = await fetch(
        `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}`,
        {
          method: 'POST',
          headers: {
            Authorization: `Bearer ${clientSecret}`,
            'Content-Type': 'application/sdp'
          },
          body: offer.sdp
        }
      ).then(r => r.text());

      await pc.setRemoteDescription({ type: 'answer', sdp: sdpAnswer });

      setStatus('listening');
      log('connected ✓ (VAD will NOT auto-reply; use Force reply or speak clearly)');
    } catch (err) {
      setStatus('error');
      log('start error: ' + (err && err.message ? err.message : err));
    }
  }

  function stopAgent() {
    try {
      if (dc && dc.readyState === 'open') dc.close();
      if (pc) pc.close();
      if (localStream) localStream.getTracks().forEach(t => t.stop());
    } catch {}
    pc = dc = localStream = null;
    setStatus('idle');
    log('stopped');
  }

  // Send a “please speak now” response (forces a TTS reply)
  function forceReply() {
    if (!dc || dc.readyState !== 'open') return log('force: channel not open');
    dc.send(JSON.stringify({
      type: 'response.create',
      response: { modalities: ['audio'], instructions: 'Please respond now.' }
    }));
    log('force reply sent');
  }

  // Simple “test sound” (agent says something)
  function testSound() {
    if (!dc || dc.readyState !== 'open') return log('test: channel not open');
    dc.send(JSON.stringify({
      type: 'response.create',
      response: { modalities: ['audio'], instructions: 'Beep played' }
    }));
    log('test sound requested');
  }

  // ===== wire up UI =====
  $('#beep-wa').addEventListener('click', () => beepWebAudio());
  $('#beep-tag').addEventListener('click', beepAudioTag);
  $('#nudge').addEventListener('click', nudgeAudio);
  $('#unlock-mic').addEventListener('click', unlockMicOnce);
  $('#start').addEventListener('click', startAgent);
  $('#stop').addEventListener('click', stopAgent);
  $('#force').addEventListener('click', forceReply);
  $('#test').addEventListener('click', testSound);

  // Optional: auto-start when URL has ?auto=1 (only useful after you’ve
  // already granted mic permission for the site at least once)
  const params = new URLSearchParams(location.search);
  if (params.get('auto') === '1') {
    (async () => {
      // small delay so autoplay has a user gesture chain from a Shortcut
      await new Promise(r => setTimeout(r, 600));
      await nudgeAudio();
      await unlockMicOnce();
      startAgent();
    })();
  }
</script>
</body>
</html>
